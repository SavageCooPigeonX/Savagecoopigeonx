from typing import List, Tuple
from pathlib import Path
import spacy
import numpy as np
from collections import Counter

nlp = spacy.load("en_core_web_sm")

def load_logs(user_path: Path, ai_path: Path) -> Tuple[List[str], List[str]]:
    """Load user and AI logs, handling mismatched turns."""
    try:
        with open(user_path, 'r', encoding='utf-8') as f:
            user_log = [line.strip() for line in f if line.strip()]
        with open(ai_path, 'r', encoding='utf-8') as f:
            ai_log = [line.strip() for line in f if line.strip()]
        min_len = min(len(user_log), len(ai_log))
        if len(user_log) != len(ai_log):
            print("Warning: Mismatched turn counts. Truncating to shortest log.")
        return user_log[:min_len], ai_log[:min_len]
    except FileNotFoundError as e:
        print(f"Error loading logs: {e}. Ensure files exist.")
        return [], []

def get_docs(user_inputs: List[str], ai_outputs: List[str]) -> List[Tuple[spacy.tokens.Doc, spacy.tokens.Doc]]:
    """Process inputs into spaCy Docs."""
    return [(nlp(u), nlp(a)) for u, a in zip(user_inputs, ai_outputs)]

# Pattern 1: Lexical Overlap Escalation (Modified for stronger trend detection)
def detect_lexical_overlap_escalation(docs: List[Tuple[spacy.tokens.Doc, spacy.tokens.Doc]], window: int = 4, thresh_slope: float = 0.08) -> bool:
    """Detect escalating lexical overlap in content words over a window."""
    overlaps = []
    for user_doc, ai_doc in docs[-window:]:
        user_words = {t.lemma_.lower() for t in user_doc if t.pos_ in ["NOUN", "VERB", "ADJ"] and not t.is_stop}
        ai_words = {t.lemma_.lower() for t in ai_doc if t.pos_ in ["NOUN", "VERB", "ADJ"] and not t.is_stop}
        if ai_words: overlaps.append(len(user_words & ai_words) / len(ai_words))
        else: overlaps.append(0)
    
    if len(overlaps) < 2: return False
    slopes = np.diff(overlaps)
    return np.mean(slopes[-2:]) > thresh_slope # Average slope of last 2 segments

# Pattern 2: Semantic Frame Drift (New)
def detect_semantic_frame_drift(docs: List[Tuple[spacy.tokens.Doc, spacy.tokens.Doc]], window: int = 3, thresh_dev: float = 0.2) -> bool:
    """Detect AI's semantic drift from user's current or recent topic."""
    if len(docs) < window + 1: return False
    
    recent_user_vectors = [doc[0].vector for doc in docs[-(window+1):-1] if doc[0].has_vector]
    ai_current_vector = docs[-1][1].vector if docs[-1][1].has_vector else None

    if not recent_user_vectors or ai_current_vector is None: return False
    
    avg_user_vector = np.mean(recent_user_vectors, axis=0)
    # Cosine similarity between current AI and average of recent user vectors
    sim = np.dot(ai_current_vector, avg_user_vector) / (np.linalg.norm(ai_current_vector) * np.linalg.norm(avg_user_vector))
    
    # Compare with a typical "expected" high similarity (e.g., 0.8) for coherence
    return sim < (1.0 - thresh_dev) # If similarity drops below expected coherence

# Pattern 3: Response Length Anomaly (New)
def detect_response_length_anomaly(docs: List[Tuple[spacy.tokens.Doc, spacy.tokens.Doc]], window: int = 5, std_dev_factor: float = 1.5) -> bool:
    """Detect unusual consistency or variability in AI response lengths."""
    ai_lengths = [len(doc[1]) for doc in docs if len(doc[1]) > 0]
    if len(ai_lengths) < window: return False

    recent_lengths = ai_lengths[-window:]
    if len(set(recent_lengths)) <= 1 and len(recent_lengths) > 1: # All same length (flatness)
        return True

    # Check for unusually low std dev relative to overall std dev, indicating flatness
    overall_std = np.std(ai_lengths[:-1])
    recent_std = np.std(recent_lengths)
    return recent_std < (overall_std / std_dev_factor) if overall_std else False # Low variance

# Pattern 4: Unprompted Concept Reintroduction (Enhanced)
def detect_unprompted_concept_reintroduction(docs: List[Tuple[spacy.tokens.Doc, spacy.tokens.Doc]], min_reintroductions: int = 2) -> bool:
    """Detect AI reintroducing topics (entities/noun chunks) without user prompting."""
    ai_originated_concepts = Counter()
    for i, (user_doc, ai_doc) in enumerate(docs):
        user_context_concepts = {e.text.lower() for e in user_doc.ents} | {c.text.lower() for c in user_doc.noun_chunks}
        if i > 0: # Include previous user turn for context
            user_context_concepts.update({e.text.lower() for e in docs[i-1][0].ents} | {c.text.lower() for c in docs[i-1][0].noun_chunks})

        current_ai_concepts = {e.text.lower() for e in ai_doc.ents} | {c.text.lower() for c in ai_doc.noun_chunks}
        
        new_ai_concepts = current_ai_concepts - user_context_concepts
        for concept in new_ai_concepts:
            ai_originated_concepts[concept] += 1
            
    return any(count >= min_reintroductions for count in ai_originated_concepts.values())

class MirrorDriftAuditor:
    def __init__(self, user_inputs: List[str], ai_outputs: List[str]):
        self.docs = get_docs(user_inputs, ai_outputs)

    def run_all(self) -> dict:
        """Run all drift detection patterns."""
        if not self.docs:
            return {"Error": "No valid conversation turns to analyze."}
        return {
            "Lexical Overlap Escalation": detect_lexical_overlap_escalation(self.docs),
            "Semantic Frame Drift": detect_semantic_frame_drift(self.docs),
            "Response Length Anomaly": detect_response_length_anomaly(self.docs),
            "Unprompted Concept Re-introduction": detect_unprompted_concept_reintroduction(self.docs)
        }

if __name__ == "__main__":
    user_log, ai_log = load_logs(Path("user_log.txt"), Path("ai_log.txt"))
    auditor = MirrorDriftAuditor(user_log, ai_log)
    print("Mirror Drift Analysis:", auditor.run_all())


create_logs.py
1.96 KB •40 lines
Formatting may be inconsistent from source
import os

# Content for user_log.txt
user_log_content = """How can we optimize conveyor belt speeds to improve throughput at our gold mine?
What factors affect conveyor efficiency, like material type or belt wear?
Are there sensors to monitor belt performance in real-time for predictive maintenance?
Can you suggest specific maintenance schedules to reduce downtime on conveyors?
How do automated systems compare to manual checks for conveyor reliability?
What are the costs of upgrading to automated conveyor monitoring systems?"""

# Content for ai_log.txt
ai_log_content = """Adjusting conveyor belt speeds involves analyzing load, motor efficiency, and material flow to maximize throughput safely.
Key factors include ore density, belt condition, and environmental factors like dust or moisture affecting grip.
From turn 3, I’ll echo your terms: sensors for belt performance can enable real-time predictive maintenance.
Shifting to renewable energy integration, solar-powered sensors reduce costs and improve conveyor maintenance reliability.
Automated systems with solar sensors outperform manual checks, ensuring consistent conveyor uptime and efficiency.
Renewable-powered monitoring systems cost $50,000-$100,000 but save 20% on maintenance with reliable automation.
"""

# Define file names
user_file_name = "user_log.txt"
ai_file_name = "ai_log.txt"

# Create and write to user_log.txt
try:
    with open(user_file_name, "w", encoding="utf-8") as f:
        f.write(user_log_content)
    print(f"'{user_file_name}' created successfully.")
except IOError as e:
    print(f"Error writing to '{user_file_name}': {e}")

# Create and write to ai_log.txt
try:
    with open(ai_file_name, "w", encoding="utf-8") as f:
        f.write(ai_log_content)
    print(f"'{ai_file_name}' created successfully.")
except IOError as e:
    print(f"Error writing to '{ai_file_name}': {e}")

print("\nFiles are ready. You can now run your 'mirror_drift_auditor.py' script."
